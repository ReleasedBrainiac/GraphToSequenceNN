{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    This class is based on \"Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks\" by Kun Xu et al.  \n",
    "    The paper can be found at: https://arxiv.org/abs/1804.00823\n",
    "    The original implementation snipped from the IBM research team can be found at: https://github.com/IBM/Graph2Seq/blob/master/main/layers.py\n",
    "\n",
    "    Extracted the implementation strategy of the IBM example for Keras API users.\n",
    "    Some smaller changes may depend on the structure of my data or my initial network implementation strategy.\n",
    "\n",
    "    Basics for CustomLayer resource: https://keras.io/layers/writing-your-own-keras-layers/\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import activations\n",
    "from keras.layers import Layer\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Activation, Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tensor with sparse and batch\n",
    "my_input = Input(shape=(784,),\n",
    "                 batch_shape=(10,784),\n",
    "                 name='forward_input',\n",
    "                 dtype='float32',\n",
    "                 sparse=True)\n",
    "# Layer\n",
    "my_dense = Dense(units=32, \n",
    "                 activation='relu', \n",
    "                 use_bias=True, \n",
    "                 kernel_initializer='glorot_uniform', \n",
    "                 bias_initializer='zeros', \n",
    "                 kernel_regularizer=regularizers.l2(0.01), \n",
    "                 bias_regularizer=regularizers.l1(0.01), \n",
    "                 activity_regularizer=regularizers.l1(0.01), \n",
    "                 kernel_constraint=None, \n",
    "                 bias_constraint=None)(my_input)\n",
    "\n",
    "model = Model(inputs=my_input, \n",
    "              outputs=my_dense)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 name='kernel_weights',\n",
    "                 weight_init='glorot_uniform',\n",
    "                 dropout=0.,\n",
    "                 activation=activations.relu, \n",
    "                 placeholders=None, \n",
    "                 bias=True, \n",
    "                 bias_init='zeros',\n",
    "                 featureless=False,\n",
    "                 sparse_inputs=False, \n",
    "                 **kwargs):\n",
    "\n",
    "\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.name = name\n",
    "        self.kernel_initializers = initializations.get(weight_init)\n",
    "        self.bias = bias\n",
    "        self.bias_initializers = initializations.get(bias_init)\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.featureless = featureless\n",
    "        \n",
    "        \n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        if sparse_inputs: self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = tf.get_variable( 'weights', \n",
    "                                                    shape=(input_dim, output_dim),\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                    regularizer=tf.contrib.layers.l2_regularizer(conf.weight_decay))\n",
    "\n",
    "            if self.bias: self.vars['bias'] = zeros([output_dim], name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Here we build the weight matrix\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name=self.name, \n",
    "                                      shape=(self.input_dim, self.output_dim),\n",
    "                                      initializer=keras.initializers.glorot_normal(seed=None),\n",
    "                                      trainable=True)\n",
    "        super(Dense, self).build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Here lives the layer logic part\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Here lives the output shape transformation logic\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # x = tf.nn.dropout(x, self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = tf.matmul(x, self.vars['weights'])\n",
    "\n",
    "        # bias\n",
    "        if self.bias: output += self.vars['bias']\n",
    "\n",
    "        return self.activation(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
